{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c493406e-7917-4149-8ad0-825b9d2e8775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from gpn.data import GenomeMSA\n",
    "from gpn.msa.inference import run_inference\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, RocCurveDisplay, average_precision_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8280a8ca-bce7-4923-bbe3-fcd65a298aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MSA...\n",
      "Loading MSA... Done\n"
     ]
    }
   ],
   "source": [
    "genome_msa = GenomeMSA(\"../../results/msa/multiz100way/89/all.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa12401b-be47-4780-82ee-372f4426e7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpn.data import Tokenizer, ReverseComplementer\n",
    "import gpn.model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "class VEPEmbedding(torch.nn.Module):\n",
    "    def __init__(self, model_path, embedding_mean=None, embedding_std=None):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_path)\n",
    "        if embedding_mean is not None and embedding_std is not None:\n",
    "            self.register_buffer(\"embedding_mean\", torch.tensor(embedding_mean))\n",
    "            self.register_buffer(\"embedding_std\", torch.tensor(embedding_std))\n",
    "        else:\n",
    "            self.embedding_mean = None\n",
    "\n",
    "    def get_embedding(self, input_ids, aux_features, exclude_pos=None):\n",
    "        embedding = self.model(\n",
    "            input_ids=input_ids, aux_features=aux_features,\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        if exclude_pos is not None:\n",
    "            embedding = torch.cat((embedding[:, :exclude_pos], embedding[:, exclude_pos+1:]), dim=1)\n",
    "\n",
    "        # chosing specific layer\n",
    "        #hidden_states = self.model(\n",
    "        #    input_ids=input_ids, aux_features=aux_features,\n",
    "        #    output_hidden_states=True,\n",
    "        #).hidden_states # note 0 is the encoder, so last layer is 12, not 11\n",
    "        #embedding = torch.cat((hidden_states[-1], hidden_states[-2]), dim=-1)  \n",
    "            \n",
    "        if self.embedding_mean is not None:\n",
    "            embedding = (embedding - self.embedding_mean) / self.embedding_std\n",
    "        \n",
    "        # this usually on\n",
    "        #embedding = embedding.reshape(len(input_ids), -1)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def get_scores(self, input_ids_ref, aux_features_ref, input_ids_alt, aux_features_alt, exclude_pos=None):\n",
    "        embedding_ref = self.get_embedding(input_ids_ref, aux_features_ref, exclude_pos=exclude_pos)\n",
    "        embedding_alt = self.get_embedding(input_ids_alt, aux_features_alt, exclude_pos=exclude_pos)\n",
    "        return torch.stack((embedding_ref, embedding_alt), dim=1)\n",
    "        return (\n",
    "            F.pairwise_distance(embedding_ref, embedding_alt),\n",
    "            (embedding_ref * embedding_alt).sum(dim=1),  # dot product\n",
    "            F.cosine_similarity(embedding_ref, embedding_alt),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids_ref_fwd=None,\n",
    "        aux_features_ref_fwd=None,\n",
    "        input_ids_alt_fwd=None,\n",
    "        aux_features_alt_fwd=None,\n",
    "        input_ids_ref_rev=None,\n",
    "        aux_features_ref_rev=None,\n",
    "        input_ids_alt_rev=None,\n",
    "        aux_features_alt_rev=None,\n",
    "    ):\n",
    "        W = input_ids_ref_fwd.shape[1]\n",
    "\n",
    "        return torch.cat((\n",
    "            self.get_scores(input_ids_ref_fwd, aux_features_ref_fwd, input_ids_alt_fwd, aux_features_alt_fwd),\n",
    "            self.get_scores(input_ids_ref_rev, aux_features_ref_rev, input_ids_alt_rev, aux_features_alt_rev),\n",
    "        ), dim=1)\n",
    "        fwd1, fwd2, fwd3 = self.get_scores(\n",
    "            input_ids_ref_fwd, aux_features_ref_fwd, input_ids_alt_fwd, aux_features_alt_fwd,\n",
    "            #exclude_pos=W//2,\n",
    "        )\n",
    "        rev1, rev2, rev3 = self.get_scores(\n",
    "            input_ids_ref_rev, aux_features_ref_rev, input_ids_alt_rev, aux_features_alt_rev,\n",
    "            #exclude_pos=(W//2) - 1,\n",
    "        )\n",
    "        return torch.stack((\n",
    "            (fwd1 + rev1) / 2,\n",
    "            (fwd2 + rev2) / 2,\n",
    "            (fwd3 + rev3) / 2,\n",
    "        \n",
    "        ), dim=1)\n",
    "\n",
    "\n",
    "class VEPEmbeddingInference(object):\n",
    "    def __init__(self, model_path, genome_msa, window_size):\n",
    "        self.model = VEPEmbedding(model_path)#, embedding_mean=embedding_mean, embedding_std=embedding_std)\n",
    "        self.genome_msa = genome_msa\n",
    "        self.window_size = window_size\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.reverse_complementer = ReverseComplementer()\n",
    "\n",
    "    def tokenize_function(self, V):\n",
    "        # we convert from 1-based coordinate (standard in VCF) to\n",
    "        # 0-based, to use with GenomeMSA\n",
    "        chrom = np.array(V[\"chrom\"])\n",
    "        pos = np.array(V[\"pos\"]) - 1\n",
    "        start = pos - self.window_size // 2\n",
    "        end = pos + self.window_size // 2\n",
    "        \n",
    "        msa_fwd, msa_rev = self.genome_msa.get_msa_batch_fwd_rev(\n",
    "            chrom,\n",
    "            start,\n",
    "            end,\n",
    "            tokenize=True,\n",
    "        )\n",
    "        pos_fwd = self.window_size // 2\n",
    "        pos_rev = pos_fwd - 1 if self.window_size % 2 == 0 else pos_fwd\n",
    "\n",
    "        ref_fwd = np.array(\n",
    "            [np.frombuffer(x.encode(\"ascii\"), dtype=\"S1\") for x in V[\"ref\"]]\n",
    "        )\n",
    "        alt_fwd = np.array(\n",
    "            [np.frombuffer(x.encode(\"ascii\"), dtype=\"S1\") for x in V[\"alt\"]]\n",
    "        )\n",
    "        ref_rev = self.reverse_complementer(ref_fwd)\n",
    "        alt_rev = self.reverse_complementer(alt_fwd)\n",
    "\n",
    "        def prepare_output(msa, pos, ref, alt):\n",
    "            ref, alt = self.tokenizer(ref.flatten()), self.tokenizer(alt.flatten())\n",
    "            input_ids, aux_features = msa[:, :, 0], msa[:, :, 1:]\n",
    "            assert (\n",
    "                input_ids[:, pos] == ref\n",
    "            ).all(), f\"{input_ids[:, pos].tolist()}, {ref.tolist()}\"\n",
    "            input_ids_alt = input_ids.copy()\n",
    "            input_ids_alt[:, pos] = alt\n",
    "            input_ids = input_ids.astype(np.int64)\n",
    "            input_ids_alt = input_ids_alt.astype(np.int64)\n",
    "            return input_ids, aux_features, input_ids_alt, aux_features\n",
    "\n",
    "        res = {}\n",
    "        (\n",
    "            res[\"input_ids_ref_fwd\"],\n",
    "            res[\"aux_features_ref_fwd\"],\n",
    "            res[\"input_ids_alt_fwd\"],\n",
    "            res[\"aux_features_alt_fwd\"],\n",
    "        ) = prepare_output(msa_fwd, pos_fwd, ref_fwd, alt_fwd)\n",
    "        (\n",
    "            res[\"input_ids_ref_rev\"],\n",
    "            res[\"aux_features_ref_rev\"],\n",
    "            res[\"input_ids_alt_rev\"],\n",
    "            res[\"aux_features_alt_rev\"],\n",
    "        ) = prepare_output(msa_rev, pos_rev, ref_rev, alt_rev)\n",
    "        return res\n",
    "\n",
    "    def postprocess(self, pred):\n",
    "        return pred\n",
    "        #return pd.DataFrame(pred, columns=[\"euclidean_distance\", \"inner_product\", \"cosine_similarity\"])\n",
    "        #return pd.DataFrame(pred, columns=[\"score\"])\n",
    "\n",
    "\n",
    "def score_vep(V2):\n",
    "    dataset = Dataset.from_pandas(V2[cols])\n",
    "    inference = VEPEmbeddingInference(\"songlab/gpn-msa-sapiens\", genome_msa, 128)\n",
    "    pred = run_inference(\n",
    "        dataset,\n",
    "        inference,\n",
    "        per_device_batch_size=256,\n",
    "        dataloader_num_workers=8,\n",
    "    )\n",
    "    return pred\n",
    "\n",
    "\n",
    "def add_scores(V2):\n",
    "    V2.loc[:, [\"euclidean_distance\", \"inner_product\", \"cosine_similarity\"]] = score_vep(V2)\n",
    "    V2.euclidean_distance *= -1\n",
    "    V2.cosine_similarity -= V2.cosine_similarity.max()\n",
    "    V2.inner_product -= V2.inner_product.max()\n",
    "    return V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb726a59-09e2-44b7-9239-a3506b2e9a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>pos</th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>pip</th>\n",
       "      <th>GPN-MSA</th>\n",
       "      <th>CADD</th>\n",
       "      <th>phyloP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>727242</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>4.651720e-12</td>\n",
       "      <td>-1.317049</td>\n",
       "      <td>-0.413109</td>\n",
       "      <td>-0.655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>758351</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>5.534810e-11</td>\n",
       "      <td>-0.050360</td>\n",
       "      <td>-0.296760</td>\n",
       "      <td>-0.289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>758443</td>\n",
       "      <td>G</td>\n",
       "      <td>C</td>\n",
       "      <td>3.978040e-12</td>\n",
       "      <td>-0.226570</td>\n",
       "      <td>-1.064142</td>\n",
       "      <td>-0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>770988</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>8.349990e-13</td>\n",
       "      <td>-0.988294</td>\n",
       "      <td>-0.170610</td>\n",
       "      <td>-0.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>796338</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>7.095540e-10</td>\n",
       "      <td>1.562877</td>\n",
       "      <td>0.140019</td>\n",
       "      <td>0.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757818</th>\n",
       "      <td>22</td>\n",
       "      <td>50783303</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.394941</td>\n",
       "      <td>-0.715689</td>\n",
       "      <td>-0.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757819</th>\n",
       "      <td>22</td>\n",
       "      <td>50784338</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>2.306390e-03</td>\n",
       "      <td>-0.965638</td>\n",
       "      <td>-0.267661</td>\n",
       "      <td>-0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757820</th>\n",
       "      <td>22</td>\n",
       "      <td>50791228</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.697440</td>\n",
       "      <td>0.163494</td>\n",
       "      <td>2.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757821</th>\n",
       "      <td>22</td>\n",
       "      <td>50791377</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.309580</td>\n",
       "      <td>-0.211272</td>\n",
       "      <td>-0.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757822</th>\n",
       "      <td>22</td>\n",
       "      <td>50792792</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>2.856520e-03</td>\n",
       "      <td>-0.059765</td>\n",
       "      <td>-0.457751</td>\n",
       "      <td>-0.020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>757823 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       chrom       pos ref alt           pip   GPN-MSA      CADD  phyloP\n",
       "0          1    727242   G   A  4.651720e-12 -1.317049 -0.413109  -0.655\n",
       "1          1    758351   A   G  5.534810e-11 -0.050360 -0.296760  -0.289\n",
       "2          1    758443   G   C  3.978040e-12 -0.226570 -1.064142  -0.252\n",
       "3          1    770988   A   G  8.349990e-13 -0.988294 -0.170610  -0.291\n",
       "4          1    796338   T   C  7.095540e-10  1.562877  0.140019   0.372\n",
       "...      ...       ...  ..  ..           ...       ...       ...     ...\n",
       "757818    22  50783303   T   C  0.000000e+00 -1.394941 -0.715689  -0.049\n",
       "757819    22  50784338   G   A  2.306390e-03 -0.965638 -0.267661  -0.450\n",
       "757820    22  50791228   G   A  0.000000e+00  1.697440  0.163494   2.096\n",
       "757821    22  50791377   T   C  0.000000e+00 -0.309580 -0.211272  -0.286\n",
       "757822    22  50792792   A   G  2.856520e-03 -0.059765 -0.457751  -0.020\n",
       "\n",
       "[757823 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\n",
    "    \"multiz100way/89/128/64/True/defined.phastCons.percentile-75_0.05_0.001/medium/0.1/42/30000/True/True/True\",\n",
    "    \"CADD.RawScore\",\n",
    "    \"phyloP\",\n",
    "]\n",
    "\n",
    "model_renaming = {\n",
    "    \"CADD.RawScore\": \"CADD\",\n",
    "    \"multiz100way/89/128/64/True/defined.phastCons.percentile-75_0.05_0.001/medium/0.1/42/30000/True/True/True\": \"GPN-MSA\",\n",
    "}\n",
    "\n",
    "V = pd.read_parquet(\"../../results/causaldb/test.parquet\")\n",
    "d = Path(f\"../../results/preds/results/causaldb\")\n",
    "for m in models:\n",
    "    model_name = model_renaming.get(m, m)\n",
    "    model_path = d / f\"{m}.parquet\"\n",
    "    V[model_name] = pd.read_parquet(model_path)[\"score\"].values\n",
    "models = [model_renaming.get(m, m) for m in models]\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d48a5e-a42b-4a67-9eda-dbedf4f48290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPN-MSA\n"
     ]
    }
   ],
   "source": [
    "exclude_abs = [\"CADD\", \"phyloP\", \"phyloP-100-vertebrates\", \"phyloP-241-mammals\", \"phyloP-Zoonomia\", 'phastCons-100-vertebrates']\n",
    "for m in models:\n",
    "    if m in V.columns and m not in exclude_abs:\n",
    "        print(m)\n",
    "        V[m] = -V[m].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2de868f1-b5bc-4e37-867e-49f0578a2e32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9860983369467541"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_na = ~V[models].isna().any(axis=1)\n",
    "not_na.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd3a268-68a3-46e3-844c-32e7bab44f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(747288, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = V[not_na]\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aedf50f-fbd1-4433-acd4-f442cff3eceb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Likely causal\n",
       "False    734816\n",
       "True       7841\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V2 = V.copy()\n",
    "V2.loc[V2.pip > 0.99, \"Likely causal\"] = True\n",
    "V2.loc[V2.pip < 0.01, \"Likely causal\"] = False\n",
    "V2 = V2.dropna(subset=\"Likely causal\")\n",
    "V2[\"Likely causal\"] = V2[\"Likely causal\"].astype(bool)\n",
    "V2[\"Likely causal\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2628107d-8524-448d-b03c-7ae45f988bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPN-MSA</td>\n",
       "      <td>0.552169</td>\n",
       "      <td>0.019186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CADD</td>\n",
       "      <td>0.520590</td>\n",
       "      <td>0.023708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phyloP</td>\n",
       "      <td>0.559628</td>\n",
       "      <td>0.018544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model     AUROC     AUPRC\n",
       "0  GPN-MSA  0.552169  0.019186\n",
       "1     CADD  0.520590  0.023708\n",
       "2   phyloP  0.559628  0.018544"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for m in models:\n",
    "    results.append([\n",
    "        m,\n",
    "        roc_auc_score(V2[\"Likely causal\"], -V2[m]),\n",
    "        average_precision_score(V2[\"Likely causal\"], -V2[m]),\n",
    "    ])\n",
    "results = pd.DataFrame(results, columns=[\"Model\", \"AUROC\", \"AUPRC\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53c38c14-2205-4db1-b190-0e6577a3bbf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='AUPRC', ylabel='Model'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "sns.pointplot(\n",
    "    data=results.sort_values(\"AUPRC\", ascending=False),\n",
    "    y=\"Model\",\n",
    "    x=\"AUPRC\",\n",
    "    join=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3aaed1-c619-4e66-889e-7d3b5e214887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpn",
   "language": "python",
   "name": "gpn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
