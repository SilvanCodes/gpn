{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gpn.phylogpn import model, tokenizer # Wrapper around HuggingFace\n",
    "\n",
    "# Example data\n",
    "seqs = [\n",
    "    \"TATAAA\",\n",
    "    \"GGCCAATCT\",\n",
    "    \"CACGTG\",\n",
    "    \"AGGTCACGT\",\n",
    "    \"GCCAGCC\",\n",
    "    \"GGGGATTTCC\"\n",
    "]\n",
    "\n",
    "# Output length is input length minus 480 (the receptive field size minus 1)\n",
    "pad_token = tokenizer.pad_token\n",
    "pad_size = 481 // 2\n",
    "pad_sequence = lambda seq: pad_token * pad_size + seq + pad_token * pad_size\n",
    "padded_seqs = [pad_sequence(seq) for seq in seqs]\n",
    "input_tensor = tokenizer(padded_seqs, return_tensors=\"pt\", padding=True)[\"input_ids\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    padded_embeddings = model.get_embeddings(input_tensor)\n",
    "    padded_logits = model(input_tensor) # These are log rate parameters for the F81 model\n",
    "\n",
    "embeddings = []\n",
    "logits = []\n",
    "\n",
    "for i in range(len(seqs)):\n",
    "    length = len(seqs[i])\n",
    "    embeddings.append(padded_embeddings[i, :length])\n",
    "    logits.append({})\n",
    "\n",
    "    for k in \"ACGT\":\n",
    "        logits[-1][k] = padded_logits[k][i, :length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2399,  0.2717,  0.1193,  ..., -0.2694,  1.2095, -0.8143],\n",
       "        [-0.3725,  1.2785, -0.0264,  ..., -0.5638, -0.1288, -1.1086],\n",
       "        [-0.7089,  0.8101, -1.5966,  ..., -0.4742,  1.5401, -0.8086],\n",
       "        [-0.6684,  1.1999, -1.6881,  ...,  0.0269, -0.1773,  0.2568],\n",
       "        [-0.1105,  0.2217, -0.3428,  ..., -0.4531,  1.1346, -0.8851],\n",
       "        [-0.8363,  1.2306,  0.3084,  ..., -0.7149,  0.5221,  0.4968]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `embeddings` is a list of tensors, one per item in the batch, each containing embeddings for each position in the sequence\n",
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': tensor([ 0.3503,  0.1109,  0.5503, -0.1649,  0.6182,  0.1694]),\n",
       " 'C': tensor([-0.0261,  0.3540, -0.3431,  0.5716,  0.4910,  0.3965]),\n",
       " 'G': tensor([0.4168, 0.4671, 0.3895, 0.1255, 0.1161, 0.3556]),\n",
       " 'T': tensor([-0.2667,  0.0614, -0.2464,  0.5956, -0.1491, -0.1738])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `logits` is a list of dictionaries, one per item in the batch, each containing the log rate parameters for the F81 model\n",
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': tensor([0.1740, 0.1369, 0.2125, 0.1039, 0.2274, 0.1452]),\n",
       " 'C': tensor([0.1218, 0.1781, 0.0887, 0.2214, 0.2042, 0.1858]),\n",
       " 'G': tensor([0.1834, 0.1929, 0.1784, 0.1370, 0.1358, 0.1725]),\n",
       " 'T': tensor([0.1250, 0.1736, 0.1276, 0.2961, 0.1406, 0.1372])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get likelihoods\n",
    "\n",
    "likelihood_list = []\n",
    "\n",
    "for logit_dict in logits:\n",
    "    logit_tensor = torch.stack([logit_dict[k] for k in \"ACGT\"])\n",
    "    likelihood_tensor = torch.softmax(logit_tensor, dim=1)\n",
    "    likelihood_dict = {k: likelihood_tensor[i] for i, k in enumerate(\"ACGT\")}\n",
    "    likelihood_list.append(likelihood_dict)\n",
    "\n",
    "likelihood_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2926)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log likelihood ratios are used to score whether a substitution is more likely under one model than another\n",
    "# For example, the log likelihood ratio of a C to T substitution at position 2 in the first sequence is:\n",
    "logits[0][\"T\"][1] - logits[0][\"C\"][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
